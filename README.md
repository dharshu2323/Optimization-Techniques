# Optimization-Techniques

# GRADIENT DESCENT:
  
   ðŸ‘‰It is the iterative optimization algorithm used in machine learning to train models.
   
   
   ðŸ‘‰It is also used to train neural networks
   
   
   ðŸ‘‰It helps in finding the local minimum of a function.


![gradient-descent-in-machine-learning1](https://github.com/dharshu2323/Optimization-Techniques/assets/104815447/08e4968e-1e68-429a-a601-30ab34231784)

# MINI-BATCH GRADIENT DESCENT:
   ðŸ‘‰In this algorithm the training dataset is divided into smaller sub-datasets.

   ðŸ‘‰Here we have Dataset Division,Iterative Update,Parameter Update  ,Convergence

   ![download (1)](https://github.com/dharshu2323/Optimization-Techniques/assets/104815447/f8493a36-30f8-49b3-bf89-c0b530c8cc70)

# ADAPTIVE GRADIENT DESCENT:
  ðŸ‘‰In this algorithm adapts learning rates for each parameter individually and incorporates exponential moving averages of gradients 
      and squared gradients.   

![download](https://github.com/dharshu2323/Optimization-Techniques/assets/104815447/d2421010-ddc6-4fa2-a71e-7a3b79d0bea8)

# GENETIC ALGORITHM
   ðŸ‘‰It is a stochastic global optimization algorithm.
   
   
   ðŸ‘‰It is a  biologically inspired algorithm

# HYPERPARAMETER TUNING

   ðŸ‘‰It involves finding the optimal combination of hyperparameters that leads to the best performance of the model on a validation set or through cross-validation. 
   
   
   ðŸ‘‰GridSearchCV and RandomizedSearchCV are the tools used for

   ![67819opt](https://github.com/dharshu2323/Optimization-Techniques/assets/104815447/8d593946-bdc1-4b62-9e12-c721aaee16df)

